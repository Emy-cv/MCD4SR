'''NDCG@20'''
import torch
import time
from accelerate import Accelerator
from data.processed import load_seq_datasets
from data.utils import batch_to
from evaluate.metrics import MetricsAccumulator
from modules.model_denoiser_main import EncoderDecoderRetrievalModel
from torch.utils.data import DataLoader
from utils import parser
from utils.collate_fn import collate_seqbatch
from utils.optimizer import build_optimizer, set_module_lr
from utils.util import *

def test(
    split_batches=True,
    mixed_precision_type="fp16",
    amp=False,
    args=None
):
    accelerator = Accelerator(
        split_batches=split_batches,
        mixed_precision=mixed_precision_type if amp else 'no'
    )
    device = accelerator.device

    # ----------------- åŠ è½½æ•°æ® -----------------
    _, _, test_dataset = load_seq_datasets(
        args=args,
        inter_file=args.inter_path,
        num_items=args.num_items,
        max_seq_len=args.max_seq_len
    )

    test_dataloader = DataLoader(
        test_dataset,
        batch_size=args.test_batch_size,
        shuffle=False,
        collate_fn=collate_seqbatch,
        num_workers=args.num_workers,
        pin_memory=True
    )

    test_dataloader = accelerator.prepare(test_dataloader)

    model = EncoderDecoderRetrievalModel(
        text_dim = args.text_dim,
        visual_dim = args.visual_dim,
        attn_dim = args.attn_embed_dim,
        dropout=args.dropout,
        attn_heads=args.attn_heads,
        encoder_layers = args.attn_layers,
        max_pos = args.max_seq_len,
        top_k=args.top_k,
        args=args
    )

    # æ²¡æœ‰è®­ç»ƒï¼Œæ‰€ä»¥ä¸éœ€è¦ optimizer
    model, _ = accelerator.prepare(model, None)

    # ----------------- åˆå§‹åŒ–æŒ‡æ ‡ -----------------
    metrics_accumulator = MetricsAccumulator()
    best_test_metrics = {
        "recall": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "ndcg": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "precision": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "mrr": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
    }

    # ----------------- åŠ è½½æœ€ä½³æ¨¡å‹ -----------------
    best_model_path = os.path.join(args.ckpts, "best_model.pt")
    if not os.path.exists(best_model_path):
        print("âŒ No best model found for standalone testing")
        return

    checkpoint = torch.load(best_model_path, map_location=device)
    model.load_state_dict(checkpoint['model'])
    model.eval()

    # ----------------- æµ‹è¯• -----------------
    test_metrics = evaluate_model(metrics_accumulator, model, test_dataloader, device, args, epoch=-1)

    if accelerator.is_main_process:
        print("\n" + "="*60)
        print("ğŸ“Š STANDALONE TEST RESULTS")
        print("="*60)
        print_test_metrics(-1, test_metrics)

        # æ›´æ–° best_test_metrics
        current_ndcg20 = test_metrics.get('ndcg@20', 0.0)
        if current_ndcg20 > best_test_metrics['ndcg'][20]:
            for metric in best_test_metrics:
                for k in [5, 10, 20, 50]:
                    metric_key = f"{metric}@{k}"
                    best_test_metrics[metric][k] = test_metrics.get(metric_key, best_test_metrics[metric][k])
            print_best_metrics(best_test_metrics, "Test", best_test_epoch=-1)

def train(
    split_batches=True,
    amp=False,
    mixed_precision_type="fp16",
    args=None
):  

    accelerator = Accelerator(
        split_batches=split_batches,
        mixed_precision=mixed_precision_type if amp else 'no'
    )

    device = accelerator.device

    # ----------------- åŠ è½½è®­ç»ƒåºåˆ—æ•°æ® -----------------
    train_dataset, eval_dataset, test_dataset = load_seq_datasets(
        args=args,
        inter_file=args.inter_path,
        num_items=args.num_items,
        max_seq_len=args.max_seq_len
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.train_batch_size, 
        shuffle=True, 
        collate_fn=collate_seqbatch,
        num_workers=args.num_workers,
        pin_memory=True             
    )
    eval_dataloader = DataLoader(
        eval_dataset, 
        batch_size=args.test_batch_size, 
        shuffle=False, 
        collate_fn=collate_seqbatch,
        num_workers=args.num_workers,
        pin_memory=True      
    )

    test_dataloader = DataLoader(
        test_dataset, 
        batch_size=args.test_batch_size, 
        shuffle=False, 
        collate_fn=collate_seqbatch,
        num_workers=args.num_workers,
        pin_memory=True      
    )

    # ä½¿ç”¨acceleratorå‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
    train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(
        train_dataloader, eval_dataloader, test_dataloader
    )

    model = EncoderDecoderRetrievalModel(
        text_dim = args.text_dim,
        visual_dim = args.visual_dim,
        attn_dim = args.attn_embed_dim,
        dropout=args.dropout,
        attn_heads=args.attn_heads,
        encoder_layers = args.attn_layers,
        max_pos = args.max_seq_len,
        top_k=args.top_k,
        args=args
    )

    optimizer = build_optimizer(
        model,
        weight_decay=args.weight_decay,
        lr_id_encoder=args.lr_encoder,
        lr_modal_encoder=args.lr_encoder,
        lr_denoiser=args.lr_encoder,
        lr_item=1e-3,
        lr_modal_proj=1e-3,
        lr_default=1e-3
    )

    # ä½¿ç”¨acceleratorå‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
    model, optimizer = accelerator.prepare(
        model, optimizer
    )

    # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ç´¯ç§¯å™¨
    metrics_accumulator = MetricsAccumulator()
    num_params = sum(p.numel() for p in model.parameters())
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    print(f"Device: {device}, Num Parameters: {num_params}")

    # åˆå§‹åŒ–å…¨å±€æœ€ä¼˜å­—å…¸
    best_eval_metrics = {
        "recall": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "ndcg": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "precision": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "mrr": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
    }
    best_test_metrics = {
        "recall": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "ndcg": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "precision": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
        "mrr": {5: 0.0, 10: 0.0, 20: 0.0, 50: 0.0},
    }

    best_eval_epoch = 0
    best_test_epoch = 0

    # ---------------------------
    # è®­ç»ƒé˜¶æ®µ
    # ---------------------------
    '''
    ä¿®æ”¹å…¨å±€æœ€ä¼˜ã€æ—©åœæœºåˆ¶
    '''
    start_time = time.time()

    if accelerator.is_main_process:
        print("\nInitial freeze done:")

    # ä¿®æ­£ï¼šæ—©åœæœºåˆ¶ç›¸å…³å˜é‡
    no_improvement_count = 0  # è¿ç»­éªŒè¯è½®æ¬¡æœªæå‡è®¡æ•°
    max_no_improvement = args.max_no_improvement   # æœ€å¤§è¿ç»­éªŒè¯è½®æ¬¡æœªæå‡
    early_stop_triggered = False
    last_eval_epoch = -1      # è®°å½•ä¸Šä¸€æ¬¡éªŒè¯çš„epoch

    print_lr_and_frozen_status(model, optimizer)
    for epoch in range(args.num_epochs):
        if early_stop_triggered:
            break  # æ—©åœè§¦å‘ï¼Œé€€å‡ºè®­ç»ƒå¾ªç¯
        # -------------------- è®­ç»ƒ --------------------
        for step, batch in enumerate(train_dataloader, start=1):
            model.train()
            optimizer.zero_grad()
            data = batch_to(batch, device)
            
            with accelerator.autocast():
                model_output = model.calculate_loss(batch=data, device=device, args=args, epoch=epoch)
                loss = model_output.loss["total_loss"]
            
            accelerator.backward(loss)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # -------------------- æŸå¤±æ‰“å°éƒ¨åˆ† --------------------
            if accelerator.is_main_process:
                total_steps = len(train_dataloader)
                print_steps = [int(total_steps * i / 5) for i in range(1, 6)]
                
                if (step + 1) in print_steps or (step + 1) == total_steps:
                    elapsed = time.time() - start_time
                    postfix_str = format_postfix({
                        "total_loss": loss.item(),
                        "item_classify": args.w_icla * model_output.loss.get('icla', 0.0),
                        "similar_w": args.w_simw * model_output.loss.get('simw', 0.0),
                        "balance_w": args.w_balw * model_output.loss.get('balw', 0.0),
                        "modal_consistency": args.w_moct * model_output.loss.get('moct', 0.0),
                    })
                    print(f"\rTrain [{epoch+1}/{args.num_epochs}] "
                        f"Step [{step+1}/{total_steps}] "
                        f"Elapsed: {format_time(elapsed)} | {postfix_str}")
        # -------------------- éªŒè¯é›†è¯„ä¼° --------------------
        if (epoch + 1) % args.eval_every == 0:
            model.eval()
            eval_metrics = evaluate_model(metrics_accumulator, model, eval_dataloader, device, args, epoch)
            
            if accelerator.is_main_process:
                print_eval_metrics(epoch + 1, eval_metrics)

            # ä¿å­˜éªŒè¯é›†æœ€ä½³æ¨¡å‹ï¼ˆå…³é”®æŒ‡æ ‡æ”¹ä¸º ndcg@5 å’Œ ndcg@10ï¼‰
            current_ndcg20 = eval_metrics.get('ndcg@20', 0.0)
            
            # æ£€æŸ¥æ˜¯å¦æå‡ï¼ˆndcg@5 å’Œ ndcg@10 éƒ½æå‡æ‰ä¿å­˜ï¼‰
            improvement_found = False
            if current_ndcg20 > best_eval_metrics['ndcg'][20]:
                improvement_found = True
                no_improvement_count = 0  # é‡ç½®æœªæå‡è®¡æ•°
                best_eval_epoch = epoch + 1
                save_model(best_eval_epoch, model, optimizer, args.experiment_path)
                print(f"\nğŸ‰ New Best Eval Model Saved at epoch {best_eval_epoch}")
                
                # ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æŒ‡æ ‡
                for metric in best_eval_metrics:
                    for k in [5, 10, 20, 50]:
                        metric_key = f"{metric}@{k}"
                        best_eval_metrics[metric][k] = eval_metrics.get(metric_key, best_eval_metrics[metric][k])
                
                print_best_metrics(best_eval_metrics, "Eval", best_eval_epoch)

                # âœ… å½“éªŒè¯é›†æœ‰æå‡æ—¶ç«‹åˆ»æµ‹è¯•
                model.eval()
                test_metrics = evaluate_model(metrics_accumulator, model, test_dataloader, device, args, epoch)
                if accelerator.is_main_process:
                    print_test_metrics(epoch + 1, test_metrics)

                current_ndcg20 = test_metrics.get('ndcg@20', 0.0)
                if current_ndcg20 > best_test_metrics['ndcg'][20]:
                    best_test_epoch = epoch + 1
                    print(f"\nğŸ‰ New Best Test Metrics Found at epoch {best_test_epoch}")
                    
                    # ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æµ‹è¯•æŒ‡æ ‡
                    for metric in best_test_metrics:
                        for k in [5, 10, 20, 50]:
                            metric_key = f"{metric}@{k}"
                            best_test_metrics[metric][k] = test_metrics.get(metric_key, best_test_metrics[metric][k])
                    
                    print_best_metrics(best_test_metrics, "Test", best_test_epoch)
            
            else:
                # æ²¡æœ‰æå‡ï¼Œè®¡æ•°å¢åŠ 
                no_improvement_count += 1
                if accelerator.is_main_process:
                    print(f"âš ï¸ No improvement for {no_improvement_count} consecutive validation evaluations")
                    print(f"Current: NDCG@20={current_ndcg20:.4f}")
                    print(f"Best:    NDCG@20={best_eval_metrics['ndcg'][20]:.4f}")

            # æ£€æŸ¥æ—©åœæ¡ä»¶
            if no_improvement_count >= max_no_improvement:
                early_stop_triggered = True
                if accelerator.is_main_process:
                    print(f"\nğŸš¨ Early stopping triggered after {epoch + 1} epochs!")
                    print(f"No improvement in NDCG@5 & NDCG@20 for {max_no_improvement} consecutive validation evaluations")
                break  # è·³å‡ºè®­ç»ƒå¾ªç¯

        # -------------------- æµ‹è¯•é›†è¯„ä¼° --------------------
        if not early_stop_triggered and (epoch + 1) % args.test_every == 0:
            model.eval()
            test_metrics = evaluate_model(metrics_accumulator, model, test_dataloader, device, args, epoch)
            
            if accelerator.is_main_process:
                print_test_metrics(epoch + 1, test_metrics)

            # ä¿å­˜æµ‹è¯•é›†æœ€ä½³æ¨¡å‹ï¼ˆå…³é”®æŒ‡æ ‡åŒæ ·æ”¹ä¸º ndcg@5 & ndcg@10ï¼‰
            current_ndcg20 = test_metrics.get('ndcg@20', 0.0)
            
            if current_ndcg20 > best_test_metrics['ndcg'][20]:
                best_test_epoch = epoch + 1
                print(f"\nğŸ‰ New Best Test Metrics Found at epoch {best_test_epoch}")
                
                # ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æµ‹è¯•æŒ‡æ ‡
                for metric in best_test_metrics:
                    for k in [5, 10, 20, 50]:
                        metric_key = f"{metric}@{k}"
                        best_test_metrics[metric][k] = test_metrics.get(metric_key, best_test_metrics[metric][k])
                
                print_best_metrics(best_test_metrics, "Test", best_test_epoch)

    # --------------------------- 
    # æœ€ç»ˆç»“æœæ€»ç»“ï¼ˆä¸éœ€è¦é‡æ–°æµ‹è¯•ï¼‰
    # ---------------------------
    if accelerator.is_main_process:
        print("\n" + "="*60)
        print("TRAINING COMPLETED")
        print("="*60)
        
        if early_stop_triggered:
            print(f"Early stopping triggered at epoch {epoch + 1}")
            print(f"No improvement for {no_improvement_count} consecutive validation cycles")
        else:
            print(f"Training finished normally at epoch {args.num_epochs}")
        
        # ç›´æ¥æ‰“å°æœ€ä½³ç»“æœï¼Œä¸éœ€è¦é‡æ–°æµ‹è¯•
        print(f"Best validation epoch: {best_eval_epoch}")
        print(f"Best test epoch: {best_test_epoch}")
        
        print("\n" + "="*60)
        print("BEST RESULTS SUMMARY")
        print("="*60)
        
        # æ‰“å°éªŒè¯é›†æœ€ä½³ç»“æœ
        print("VALIDATION SET (Best):")
        print_best_metrics(best_eval_metrics, "Eval", best_eval_epoch)
        
        # æ‰“å°æµ‹è¯•é›†æœ€ä½³ç»“æœ
        print("\nTEST SET (Best):")
        print_best_metrics(best_test_metrics, "Test", best_test_epoch)
        
        # è®­ç»ƒç»Ÿè®¡
        print("\n" + "="*60)
        print("TRAINING STATISTICS")
        print("="*60)
        print(f"Total epochs trained: {epoch + 1}")
        print(f"Early stopping: {'Yes' if early_stop_triggered else 'No'}")
        if early_stop_triggered:
            print(f"Stopped after {no_improvement_count} validation cycles without improvement")
        print(f"Final validation NDCG@20: {best_eval_metrics['ndcg'][20]:.4f}")
        print(f"Final test NDCG@20: {best_test_metrics['ndcg'][20]:.4f}")


if __name__ == "__main__":
    # config
    args = parser.get_args()
    parser.setup(args)
    if getattr(args, "test", False):
        test(args=args)
    else:
        train(args=args)
